Just like virtualizing CPU, in order to virtualize memory efficiently and securely, we need both mechanism and policy.

The mechanism for virtualizing memory is called **address translation**.

- [The Abstraction: Address Space](#the-abstraction-address-space)
  - [The Address Space](#the-address-space)
  - [Goals](#goals)
    - [Transparency](#transparency)
    - [Efficiency](#efficiency)
    - [Protection](#protection)
- [Mechanism: Address Translation](#mechanism-address-translation)
- [Dynamic (Hardware-based) Relocation](#dynamic-hardware-based-relocation)
  - [Hardware Support: A Summary](#hardware-support-a-summary)
  - [Operating System Issues](#operating-system-issues)
- [Segmentation](#segmentation)
- [Paging](#paging)
  - [Introduction to Paging](#introduction-to-paging)
  - [Two Problems of Paging](#two-problems-of-paging)
    - [Page Table Can Be Very Big](#page-table-can-be-very-big)
    - [Paging Is Too Slow](#paging-is-too-slow)
  - [Paging: Faster Translations (TLBs)](#paging-faster-translations-tlbs)
    - [Introduction to TLB](#introduction-to-tlb)
    - [TLB Basic Algorithm](#tlb-basic-algorithm)
    - [Who Handles The TLB Miss?](#who-handles-the-tlb-miss)
    - [TLB Contents: What's In There?](#tlb-contents-whats-in-there)
    - [TLB Issue: Context Switches](#tlb-issue-context-switches)
    - [Issue: Replacement Policy](#issue-replacement-policy)
  - [Paging: Smaller Tables](#paging-smaller-tables)
    - [The Problem](#the-problem)
    - [Simple Solution: Bigger Pages](#simple-solution-bigger-pages)
    - [Hybrid Approach: Paging and Segments](#hybrid-approach-paging-and-segments)
    - [Multi-level Page Tables](#multi-level-page-tables)
- [Beyond Physical Memory: Mechanisms](#beyond-physical-memory-mechanisms)
  - [Swap Space](#swap-space)
  - [The Present Bit](#the-present-bit)
  - [The Page Fault](#the-page-fault)
  - [What If Memory Is Full?](#what-if-memory-is-full)
  - [When Replacements Really Occur](#when-replacements-really-occur)
- [Page Replacement Policies](#page-replacement-policies)
  - [The Optimal Replacement Policy (Impossible to implement in reality)](#the-optimal-replacement-policy-impossible-to-implement-in-reality)
  - [FIFO](#fifo)
  - [Random](#random)
  - [Using History: LRU](#using-history-lru)
    - [Looping Sequential Workload](#looping-sequential-workload)
  - [Approximating LRU](#approximating-lru)
  - [Considering Dirty Pages](#considering-dirty-pages)
  - [Thrashing](#thrashing)

# The Abstraction: Address Space

One way to implement time sharing would be to run process for a short while, giving it full access to all memory, then stop it, save all of its state to some kine of disk (including all of physical memory), load other process's state, run it for a while, and thus implement some kind of crude sharing of the machine.

Unfortunately, this approach has a big problem: it is way too slow, particularly as memory grows. While saving and restoring register-level state (the PC, general-purpose registers, etc.) is relatively fast, saving the entire contents of memory to disk is brutally non-performant. Thus, what we'd rather do is leave processes in memory while switching between them, allowing the OS to implement time sharing efficiently.

Allowing multiple programs to reside concurrently in memory makes **protection** an important issue; you don't want a process to be able to read, or worse, write some other process's memory.

## The Address Space

In order to create an **easy to use** abstraction of physical memory. We call this abstraction the **address space**, and it is the running program's view of memory in the system.

The address space of a process contains all of the memory state of the running program. The **code** of the program (the instructions) have to live in memory somewhere, and thus they are in the address space. The program, while it is running, uses a **stack** to keep track of where it is in the function call chain as well as to allocate local variables and pass parameters and return values to and from routines. Finally, the **heap** is used for dynamically-allocated, user-managed memory, such as that you might receive from a call to `malloc()` in C or `new` in ann object-oriented language such as C++ or Java.

Note that **address space** is an **abstraction** that the OS is proving to the running program. The program doesn't start in memory at physical address 0; rather it is loaded at some arbitrary physical address(es).

When the OS does this, we say the OS is **virtualizing memory**, because the running program thinks it is loaded into memory at a particular address (say 0) and has a potentially very large address space (say 32-bits or 64-bits); the reality is quite different.

When process A tries to perform a load at address 0 (which we will call a **virtual address**), somehow the OS, in tandem with some hardware support, will have to make sure the load doesn't actually go to physical address 0 but rather to physical address where the data resides. This is the key to virtualization of memory, which underlies every modern computer system in the world.

## Goals

Three goals of virtualizing memory:

1. Transparency
1. Efficiency
1. Protection

### Transparency

The OS should implement virtual memory in a way that is invisible to the running program. Thus, the program shouldn't be aware of the fact that memory is virtualized; rather, the program behaves as if it has its own private physical memory. Behind the scenes, the OS (and hardware) does all the work to multiplex memory among many different jobs, and hence implements the illusion.

### Efficiency

The OS should strive to make the virtualization as **efficient** as possible, both in terms of time and space. In implementing time-efficient virtualization, the OS will hav eto rely on hardware support, including hardware features such as TLBs.

### Protection

The OS should make sure to **protect** processes from one another as well as the OS itself from processes. When one process performs a load, a store, or an instruction fetch, it should not be able to access or affect in any way the memory contents of any other process or the OS itself (that is, anything _outside_ its address space).Protection thus enables us to deliver the property of **isolation** among processes; each process should be running in its own isolated cocoon, safe from the ravages of other faulty or even malicious processes.

# Mechanism: Address Translation

The generic technique we will use, which you can consider an addition to our general approach of limited direct execution, is something that is referred to as **hardware-based address translation**, or just **address translation** for short. With address translation, the hardware transforms each memory access, changing the **virtual** address provided by the instruction to a **physical** address where the desired information is actually located.

Of course, the hardware alone cannot virtualize memory, as it just provides the low-level mechanism for doing so efficiently. The OS must get involved at key points to set up the hardware so that the correct translations take place; it might thus **manage memory**, keeping track of which locations are free and which are in use, and judiciously intervening to maintain control over how memory is used.

The goal of all this work is to create a beautiful **illusion**: that the program has its own private memory, where its own code and data resides. Behind that virtual reality lies the ugly physical truth: that many programs are actually sharing memory at the same time, as the CPU (or CPUs) switches between running one program and the next. THrough virtualization, the OS (with the hardware's help) turns the ugly machine reality into a useful, powerful, and easy to use abstraction.

Let's talk about different ways to virtualize memory.

# Dynamic (Hardware-based) Relocation

The first incarnation of hardware-based address translation is a simple idea referred to as **base and bounds**; the technique is also referred to as **dynamic relocation**.

Specifically, we'll need two hardware registers within each CPU: one is called the **base** register, and the other the **bounds** (sometimes called a **limit** register). This base-and-bounds pair is going to allow use to place the address space anywhere we'd like in physical memory, and do so while ensuring that the process can only access its own address space.

In this setup, each program is written and compiled as if it is loaded at address zero. However, when a program starts running, the OS decides where in physical memory it should be loaded and sets the base register to that value.

When any memory reference is generated by the process, it is **translated** by the processor in the following manner:

`physical address = virtual address + base`

Each memory reference generated by the process is a **virtual address**; the hardware in turn adds the contents of the base register to this address and the result is a **physical address** that can be issued to the memory system.

Transforming a virtual address into a physical address is exactly the technique we refer to as **address translation**; that is, the hardware takes a virtual address the process thinks it is referencing and transforms it into a physical address which is where the data actually resides. Because this relocation of the address happens at run time, and because we can move address spaces even after the process has started running, the technique is often referred to as **dynamic relocation**.

We should note that the base and bounds registers are hardware structures kept on the chip (one pair per CPU). Sometimes people call the part of the processor that helps with address translation the **memory management unit(MMU)**.

## Hardware Support: A Summary

First, we need two different CPU modes. The OS runs in **privileged mode** (or **kernel mode**), where it has access to the entire machine; applications run in **user mode**, where they are limited in what they can do. A single bit, perhaps stored in some kind of **processor status word**, indicating which mode the CPU is currently running in; upon certain special occasions(e.g., a system call or some other kind of exception or interrupt), the CPU switches mode.

Second, the hardware must also provide the **base and bounds registers** themselves; each CPU thus has an additional pair of registers, part of the **memory management unit(MMU)** of the CPU.

Third, the hardware should provide special instructions to modify the base and bounds registers, allowing the OS to change them when different processes run. These instructions are **privileged**; only in kernel (or privileged) mode can the registers be modified.

Finally, the CPU must be able to generate **exceptions** in situations where a user program tries to access memory illegally; in the case of an address that is "out of bound", the CPU should stop executing the user program and arrange for the OS "out-of-bounds" **exception handler** to run. Thus, the CPU also must provide a method to inform it of the location of these exception handlers; a few more privileged instructions are thus needed.

## Operating System Issues

There are a few critical junctures where the OS must get involved to implement our base-and-abounds version of virtual memory.

1. First, the OS must take action when a process is created, finding space for its address space in memory. When a new process is created, the OS will have to search a data structure (often called a **free list**) to find room for the new address space and then mark it used.
1. Second, the OS must do some work when a process is terminated, reclaiming all of its memory for use in other processes or the OS. Upon termination of a process, the OS thus puts its memory back on the free list, and cleans up any associated data structures as need be.
1. Third, the OS must also perform a few additional steps when a context switch occurs. There is only one base and bounds register pair on each CPU, after all, and their values differ for each running program, as each program is loaded at a different physical address in memory. Thus, the OS must _save and restore_ the base-and-bounds pair when it switches between processes. Specifically, when the OS decides to stop running a process, it must save the values of the base and bounds registers to memory, in some per-process structure such as the **process structure** or **process control block**(PCB). Similarly, when then OS resumes a running process (or runs it the first time), it must set the values of the base and bounds on the CPU to the correct values for this process.
1. Fourth, the OS must provide **exception handlers**; the OS installs these handlers at boot time (via privileged instructions).

# Segmentation

With the base and bounds registers, the OS can easily relocate processes to different parts of physical memory However, you might have noticed something interesting about these address spaces of ours: there is a big chunk of "free" space right in the middle, between the stack and the heap.

The simple approach of using a base and bounds register pair to virtualize memory is wasteful. It also makes it quite hard to run a program when the entire address space doesn't fit into memory; thus, base and bounds is not as flexible as we would like.

To solve this problem, an idea was born, and it is called **segmentation**. The idea is simple: instead of having just one base and bounds pair in our MMU, why not have a base and bounds pair per logical **segment** of the address space? A segment is just a contiguous portion of the address space of a particular length, and in our canonical address space, we have three logically-different segments: code, stack, and heap. What segmentation allows the OS to do is to place each one of those segments in different parts of physical memory, and thus avoid filling physical memory with unused virtual address space.

However, as we learned, allocating variable-sized segment in memory leads to some problem that we'd like to overcome:

1. First is external fragmentation. Because segments are variable-sized, free memory gets chopped up into odd-sized pieces, and thus satisfying a memory-allocation request can be difficult.
1. The second and perhaps more important problem is that segmentation still isn't flexible enough to support our fully generalized, sparse address space. For example, if we have a large but sparsely-used heap all in one logical segment, the entire heap must still reside in memory in order to be accessed. In other words, if our model of how the address space is being used doesn't exactly match how the underlying segmentation has been designed to support it, segmentation doesn't work very well.

# Paging

## Introduction to Paging

Chopping things up in to _variable-sized_ pieces, as we saw with **segmentation** in virtual memory, has inherent difficulties. In particular, when dividing a space into different=size chunks, the space itself can become **fragmented**, and thus allocation becomes more challenging over time.

Thus, it may be worth considering the second approach: to chop up space into _fixed-sized_ pieces. In virtual memory, we call this idea **paging**. Instead of splitting up a process's address space into some number of variable-sized logical segments (e.g., code, heap, stack), we divide it into fixed-sized units, each of which we call a **page**. Correspondingly, we view physical memory as an array of fixed-sized slots called **page frames**; each of these frames can contain a singel virtual-memory page.

Probably the most important improvement will be _flexibility_: with a fully-developed paging approach, the system will be able to support the abstraction of an address space effectively, regardless of how a process uses the address space; we won't make assumptions about the direction the heap and stack grow and how they are used. Another advantage is the _simplicity_ of free-space management that paging affords. Perhaps the OS keeps a **free list** of all free pages for and just grabs the first free page off of this list.

To record where each virtual page of the address space is placed in physical memory, the OS usually keeps a _per-process_ data structure known as a **page table**. The major role of the page table is to store **address translations** for each of the virtual page of the address space, thus letting us know where in physical memory each page resides.

It is important to remember that this page table is a _per-process_ data structure. If another process were to run, the OS would have to manage a different page table for it, as its virtual pages obviously map to _different_ physical pages (modulo any sharing going on).

To **translate** a virtual address that the process generated, we have to first split it into two components: the **virtual page number (VPN)**, and the **offset** within the page.

The process of address translation goes like this: with our virtual page number, we can now index our page table and find this virtual page's **physical frame number (PFN)** (also sometimes called the **physical page number** or **PPN**). Thus, we can translate a virtual address by replacing the VPN with the PFN and then issue tha load to physical memory.

Note the offset stays the same (i.e., it is noe translated), because the offset just tells us which byte _within_ the page we want.

## Two Problems of Paging

However there are two problems surrounding paging:

1. Page Table Can Be Very Big
1. Paging Can Be Very Slow

### Page Table Can Be Very Big

Page tables can ge terrible large, much bigger than the small segment table or base/bounds pair we have discussed previously. For example, imagine a typical 32-bit address space, with 4KB pages. This virtual address splits into a 20-bit VPN and 12-bit offset.

A 20-bit VPN implies that there are 2<sup>20</sup> translations that the OS would have to manage for each process; assuming we need 4bytes per **page table entry(PTE)** to hold the physical translation plus any other useful stuff, we get an immense 4MV of memory needed for each page table! This ia pretty large. Now imagine there are 100 processes running: this means the OS would need 400MB of memory just for all those address translations!

The page table is just a data structure that is used to map virtual addresses (or really, virtual page numbers) to physical address (physical frame numbers). Thus, any data structure could work. The simplest form is called a **linear page table**, which is just an array. The OS _indexes_ the array by the virtual page number (VPN), and looks up the page-table entry (PTE) at that index in order to find the desired physical frame number (PFN).

As for the contents of each PTE, we have a number of different bits in there worth understanding at some level:

1. A **valid bit** is common to indicate whether the particular translation is valid; for example, when a program starts running, it will have code and heap at one end of its address space, and the stack at the other. All the unused space in-between will be marked **invalid**, and if the process tries to access such memory, it will generate a trap to the OS which will likely terminate the process. Thus, the valid bit is crucial for supporting a sparse address space; by simply marking all teh unused pages in the address space invalid, we remove the need to allocate physical frames for those pages and thus save a great deal of memory.
1. We also might have **protection bits**, indicating whether the page could be read from, written to, or executed from.
1. A **present bit** indicates whether this page is in physical memory or on disk (i.e., it has been **swapped out**).
1. A **dirty bit** is also common, indicating whether the page has been modified since it was brought into memory.
1. A **reference bit** (a.k.a **accessed bit**) is sometimes used to track whether a page has been accessed, and is useful in determining which pages are popular and thus should be kept in memory; such knowledge is critical during **page replacement**.

### Paging Is Too Slow

For every memory reference (whether an instruction fetch or an explicit load or store), paging requires us to perform one extra memory reference in order to first fetch the translation from the page table.That is a lot of work! Extra memory references are costly, and in this case will likely slow down the process by a factor of two or more.

## Paging: Faster Translations (TLBs)

### Introduction to TLB

By chopping the address space into small, fixed-sized units (i.e., pages), paging requires a large amount of mapping information. because that mapping information is generally stored in physical memory, paging logically requires an extra memory lookup for each virtual address generated by the program. Going to memory for translation information before every instruction fetch or explicit load or store is prohibitively slow.

To speed address translation, we are going to add what is called (for historical reasons) a **translation-lookaside buffer**, or **TLB**. A TLB is part of the chip's **memory-management unit (MMU)**, and is simply a hardware **cache** of popular virtual-to-physical address translations; thus, a better name would be an **address-translation cache**. Upon each virtual memory reference, the hardware first checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) _without_ having to consult the page table (which has all translations). Because of their tremendous performance impact, TLBs in a real sense make virtual memory possible.

### TLB Basic Algorithm

1. First, extract the virtual page number (VPN) from the virtual address, and check if the TLB holds the translation for this VPN. If it does, we have a **TLB hit**, which means the TLB holds the translation. Success! We can now extract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory, assuming protection checks do not fail.
1. If the CPU does not find the translation in the TLB (a **TLB Miss**), we have some more work to do. The hardware accesses the page table to find the translation, and, assuming that the virtual memory reference generated by the process is valid and accessible, updates the TLB with the translation. These set of actions are costly, primarily because of the extra memory reference needed to access the page table.
1. Finally, once the TLB is updated, the hardware retries the instruction; this time, the translation is found in the TLB, and the memory reference is processed quickly.

The TLB, like all caches, is built on the premise that in the common case, translations are found in the cache (i.e., are hits). IF so, little overhead is added, as the TLB is found near the processing core and is designed to be quite fast. When a miss occurs, the hight cost of paging is incurred; the page table must be accessed to find the translation, and an extra memory reference (or more, with more complex page tables) results. If this happens often, the program will likely run noticeable more slowly; memory accesses, relative to most CPU instructions, are quite costly, and TLB misses lead to more memory accesses.

### Who Handles The TLB Miss?

In ths olden days, the hardware had complex instruction sets (sometimes called **CISC**, for complex-instruction set computers) and the people who built the hardware didn't must trust those sneaky OS people. Thus the hardware would handle the TLB miss entirely. TO do this, the hardware has to know exactly _where_ the page tables are located in memory (via a **page-table base register**), as well as their _exact format_; on a miss, the hardware would "walk" the page table, find the correct page-table entry and extract the desired translation, update the TLB with the translation, and retry the instruction.

With modern architectures, both **RISC** or (reduced-instruction set computers) have what is known as a **software-managed TLB**. ON a TLB miss, the hardware simply raises an exception, which pauses the current instruction stream, raises the privilege level to kernel mode, and jumps to a **trap handler**. A you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB misses. When run, the code will lookup the translation in the page table, use special "privileged" instructions to update the TLB, and return from the trap; at this point, the hardware retries the instruction (resulting in a TLB hit).

Let's discuss a couple of important details:

1. First, the return-from-trap instruction needs to be a little different than the return-from-trap we saw before when servicing a system call. In the latter case, the return-from-trap should resume execution at the instruction _after_ the trap into the OS, just as a return from a procedure call returns to the instruction immediately following the call into the procedure. In the former case, when returning from a TLB miss-handling trap, the hardware must resume execution at the instruction that _caused_ the trap; this retry thus lets the instruction run again, this time resulting in a TLB hit. Thus, depending on how a trap or exception was caused, the hardware must save a different PC when trapping into the OS, in order to resume properly when the time to do so arrives.
1. Second, when running the TLB miss-handling code, the OS needs to be extra careful not to cause an infinite chain of TLB misses to occur. Many solutions exist; for example, you could keep TLB miss handlers in physical memory (where they are **unmapped** and not subject to address translation), or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself; these **wired** translations always hit in the TLB.
1. The primary advantage of the software-managed approach is _flexibility_: the OS can use any data structure it wants to implement the page table, without necessitating hardware change. Another advantage is _simplicity_, the hardware doesn't do much on a miss: just raise an exception and let the OS TLB miss handler do the rest.

### TLB Contents: What's In There?

A typical TLB might have 32, 64, or 128 entries and be what is called **fully associative**. Basically, this just means that any given translation can be anywhere in the TLB, and that the hardware will search the entire TLB in parallel to find the desired translation.

The TLB commonly has a **valid** bit, which says whether the entry has a valid translation or not. Also common are **protection** bits, which determine how a page can be accessed (as in the page table). There may also be a few other fields, including an **address-space identifier**, a **dirty bit**, and so forth.

### TLB Issue: Context Switches

The TLB contains virtual-to-physical translations that are only valid for the currently running process; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) must be careful to ensure that the about-to-be-run process does not accidentally use translations from some previously run process.

There are a number of possible solutions to this problem. One approach is to simply **flush** the TLB on context switches, thus emptying it before running the next process. On a software-based system, this can be accomplished with an explicit (and privileged) hardware instruction; with a hardware-managed TLB, the flush could be enacted when the page-table base register is changed (note the OS must change the PTBR on a context switch anyhow). In either case, the flush operation simply sets all valid bits to 0, essentially clearing the contents of the TLB. However, there is a case: each time a process runs, it must incur TLB misses as it touches its data and code pages. If the OS switches between processes frequently, this cost may be high.

To reduce this overhead, some systems add hardware support to enable sharing of the TLB across context switches. In particular, some hardware systems provide an **address space identifier (ASID)** field in the TLB. You can think of the ASID as a **process identifier (PID)**, but usually it has fewer bits.

Thus, with address-space identifiers, the TLB can hold translations from different processes at the same time without any confusion. Of course, the hardware also needs to know which process is currently running in order to perform translations, and thus the OS must, on a context switch, set some privileged register to the ASID of the current process.

### Issue: Replacement Policy

As with any cache, and thus also with the TLB, one more issue that we must consider is **cache replacement**. Specifically, when we are installing a new entry in the TLB, we have to **replace** an old one, and thus the question: which one to replace?

Which TLB entry should be replaced when we add a new TLB entry? The goal, of course, being to minimize the **miss rate** (or increase **hit rate**) and thus improve performance.

We will study such policies in some detail when we tackle the problem of swapping pages to disk. A few typical policies include **least-recently-used** or **LRU**. LRU tries to take advantage of locality in the memory-reference stream, assuming it is likely that an entry that has not recently been used is a good candidate for eviction. Another typical approach is to use a **random** policy, which evicts a TLB mapping at random. Such a policy is useful due to its simplicity and ability to avoid corner-case behaviors.

## Paging: Smaller Tables

### The Problem

We now tackle the second problem that paging introduces: page tables are too big and thus consume too much memory.

Note that we usually have one page table _for every process_ in the system! With a hundred active processes, we will be allocating a lot of memory just for page tables!

> The CRUX: How To Make Page Tables Smaller?
>
> Simple array-based page tables (usually called linear page tables) are too big, taking up far too much memory on typical systems. How can we make page tables smaller? What are the key ideas? What inefficiencies arise as a result of these new data structures?

### Simple Solution: Bigger Pages

We could reduce the size of the page table in one simple way: use bigger pages. Take our 32-bit address space again, but this time assume 16KB pages. We would thus have a 18-bit VPN plus a 14-bit offset. Assuming the same size for each PTE (4 bytes), we now have 2<sup>18</sup> entries in our linear page table and thus a total size of 1MB per page table, a factor of four reduction in size of the page table.

The major problem with this approach, however, is that big pages lead to waste _within_ each page, a problem known as **internal fragmentation** (as the waste is **internal** to the unit of allocation). Applications thus end up allocating pages but only using little bits and pieces of each, and memory quickly fills up with these overly-large pages. Thus, most systems use relatively small page sizes in he common case: 4KB or 8KB.

### Hybrid Approach: Paging and Segments

For many processes, _most_ of the page table is unused, full of **invalid** entries. What a waste! Thus, the hybrid approach: instead of having a single page table for the entire address space of the process, why not have one per logical segment?

Now, remember with segmentation, we had a **base** register that told us where each segment lived in physical memory, and a **bound** or **limit** register that told us the size of said segment. In our hybrid, we still have those structures in the MMU; here, we use the base not to point to the segment itself but rather to hold the _physical address of the page table_ of that segment. The bounds register is used to indicated the end of the page table (i.e., how many valid pages it has).

However, as you might notice, this approach is not without problems:

1. First, it still requires us to use segmentation; as we discussed before, segmentation is not quite as flexible as we would like, as it assumes a certain usage pattern of the address space; if we have a large but sparsely-used heap, for example, we can still end up with a lot of page table waste.
1. Second, this hybrid causes external fragmentation to arise again. While most of memory is managed in page-sized units, page tables now can be of arbitrary size (in multiples of PTEs). Thus, finding free space for them in memory is more complicated.

### Multi-level Page Tables

A different approach doesn't rely on segmentation but attacks the same problem: how to get rid of all those invalid regions in the page table instead of keeping them all in memory? We call this approach a **multi-level page table**, as it turns the linear page table into something like a tree.

THe basic idea behind a multi-level page table is simple. First, chop up the page table into page-sized units; then, if an entire page of page-table entries(PTEs) is invalid, don't allocate that page of th page table at all. To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the **page directory**. The page directory thus either can be used to tell you where a page of the page table is, or that the entire page of the page table contains no valid pages.

The page directory, in a simple two-level table, contains one entry per page of the page table. It consists of a number of **page directory entries(PDE)**. A PDE (minimally) has a **valid bit** and a **page frame number** (PFN), similar to a PTE. However, the meaning of this valid bit is slightly different: if the PDE is valid, it means that at least one of the pages of the page table that the entry points to (via the PFN) is valid, i.e., in at least one PTE on that page pointed to by this PDE, the valid bit in that PTE is set to one. If the PDE is not valid, the rest of the PDE is not defined.

Multi-level Page Tables have some obvious advantages over approaches we've seen thus far:

1. First, and perhaps most obviously, the multi-level table only allocated page-table space in proportion to the amount of address space you are using: thus it is generally compact and supports sparse address spaces.
1. Second, if carefully constructed, each portion of the page table fits neatly within a page, making it easier to manage memory; the OS can simply grab the next free page when it needs to allocate or grow a page table.

However, there are some cons about multi-level tables:

1. It should be noted that there is a cost to multi-level tables; on a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the page directory, and one for the PTE itself), in contrast to just one load with a linear page table.
1. Another obvious negative is _complexity_. Whether it is the hardware or OS handling the page-table lookup (on a TLB miss), doing so is undoubtedly more involved than a simple linear page-table lookup.

# Beyond Physical Memory: Mechanisms

Thus far, we have assumed that all pages reside in physical memory. However, to support large address spaces, the OS will need a place to stash away portions of address spaces that currently aren't in great demand. In modern systems, this role is usually served by a **hard disk drive**.

Beyond just a single process, the addition of swap space allows the OS to support the illusion of a large virtual memory for multiple concurrently-running processes. The invention multiprogramming (running multiple programs "at once", to better utilize the machine) almost demanded the ability to swap out some pages, as early machines clearly could not hold all the pages needed by all processes at once. Thus, the combination of multiprogramming and ease-of-use leads us to want to support using more memory than is physically available.

## Swap Space

The first thing we will need to do is to reserve some space on the disk for moving pages back and forth. In operating systems, we generally refer to such space as **swap space**, because we _swap_ pages out of memory to it and _swap_ pages into memory from it. Thus, we will simply assume that the OS can read from and write to the swap space, in page-sized units. To do so, the OS will need to remember the **disk address** of a given page.

The size of the swap space is important, as ultimately it determines the maximum number of memory pages that can be in use by a system at a given time.

We should note that swap space is not the only on-disk location for swapping traffic. For example, assume you are running a program binary (e.g., `ls`, or your own compiled `main` program). The code pages from this binary are initially found on disk, and when the program runs, they are loaded into memory (either all at once when the program starts execution, or, as in modern systems, one page at a time when needed). However, if the system needs to make room in physical memory for other needs, it can safely re0use the memory space for these code pages, knowing that it can later swap them in again from the on-disk binary in the file system.

## The Present Bit

Recall first what happens on a memory reference. The running process generates virtual memory references (for instruction fetches, or data accesses), and, in this case, the hardware translates them into physical addresses before fetching the desired data from memory.

Remember that the hardware first extracts the VPN from the virtual address, checks the TLB for a match (a **TLB hit**), and if a hit, produces the resulting physical address and fetches it from memory. This is hopefully the common case, as it is fast (requiring no additional memory accesses).

If the VPN is not found in the TLB (i.e., a **TLB miss**), the hardware locates the page table in memory (using the **page table base register**) and looks up the **page table entry (PTE)** for this page using the VPN as an index. If the page is valid and present in physical memory, the hardware extracts the PFN from the PTE, installs it in the TLB, and retries the instruction, this time generating a TLB hit; so far, so good.

If we wish to allow pages to be swapped to disk, however, we must add even more machinery. Specifically, when the hardware looks in the PTE, it may find that the page is _not present_ in physical memory. The way the hardware (or the OS, in a software-managed TLB approach) determines this is through a new piece of information in each page-table entry, known as the **present bit**. If the present bit is set to one, it means the page is present in physical memory and every thing proceeds as above; if it is set to zero, the page is _not_ in memory but rather on disk somewhere. The act of accessing a page that is not in physical memory is commonly refereed to as a **page fault**.

Upon a page fault, the OS is invoked to service tha page fault. A particular piece of code, known as a **page-fault handler**, runs, and must service tha pge fault.

## The Page Fault

If a page is not present, the OS is put in charge to handle the page fault. The appropriately-named OS **page-fault handler** runs to determine what to do. Virtually all systems handle page faults in software; even with a hardware-managed TLB, the hardware trusts the OS to manage this important duty.

A question arises: how will the OS know where to find the desired page? In many systems, the page table is a natural place to store such information. Thus, the OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to find the address, and issues the request to disk to fetch the page into memory.

When the disk I/O completes, the OS will the update the page table to mark the page as present, update the PFN field of the page-table entry (PTE) to record the in-memory location of the newly-fetched page, and retry the instruction. This next attempt may generate a TLB miss, which would then be serviced and update the TLB with the translation (one could alternately update the TLB when servicing the page fault to avoid this tep). Finally, a last retry would find the translation in the TLB and thus proceed to fetch the desired data or instruction from memory at the translated physical address.

Note that while the I/O is in flight, the process will be in the **blocked** state. Thus, the OS will be free to run other ready processes while the page fault is being serviced. Because I/O is expensive, this **overlap** of the I/O (page fault) of one process and the execution of another is yet another way a multiprogrammed system can make the most effective use of its hardware.

## What If Memory Is Full?

Memory may be full. Thus, the OS might like to first **page out** one or more pages to make room for the new page(s) the OS is about to bring in. The process of picking a page to kick out, or **replace** is known as the **page-replacement policy**.

## When Replacements Really Occur

To keep a small amount of memory free, most OS thus have some kind of **high watermark** and **low watermark** to help decide when to start evicting pages from memory. How this works is as follows: when the OS notices that there are fewer than LW pages available, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are HW pages available. The background thread, sometimes called the **swap daemon** or **page daemon**, then goes to sleep, happy the it has freed some memory for running processes and the OS to use.

By performing a number of replacements at once, new performance optimizations become possible. For example, many systems will **cluster** or **group** a number of pages and write them out at once to the swap partition, thus increasing the efficiency of the disk.

# Page Replacement Policies

## The Optimal Replacement Policy (Impossible to implement in reality)

Replaces the page that will be access _furthest in the future_ is the optimal policy, resulting in the fewest-possible cache misses.

## FIFO

Simple.

## Random

Also simple.

## Using History: LRU

Unfortunately, any policy as simple as FIFO or Random is likely to have a common problem: it might kick out an important page, one that is about to be referenced again.

As we did with scheduling policy, to improve our guess at the future, we once again lean on the past and use _history_ as our guide.

One type of historical information a page-replacement policy could use is frequency; if a page has been access many times, perhaps it should not be replaced as it clearly has some value. A more commonly-used property of a page is its **recency of access**; the more recently a page has been accessed, perhaps the more likely it will be accessed again.

And thus, a family of simple historically-based algorithms are born. The **Least-Frequently-Used** (LFU) policy replaces the least-frequently-used page when an eviction must take place. Similarly, the **Least-Recently-Used** (LRU) policy replaces the least-recently-used page.

### Looping Sequential Workload

Looping Sequential Workload Example: we refer to 50 pages in sequence, starting at 0, then 1, ..., up to page 49, and then we loop, repeating those accesses, for a total of 10,000 accesses to 50 unique pages.

This workload, common in many applications, represent a worst-case for both LRU and FIFO. These algorithms, under a looping-sequential workload, kick out older pages; unfortunately, due to the looping nature of the workload, these older pages are going to be accessed sooner than the pages that the policies prefer to keep in cache.

If our cache is of size 49, a looping-sequential workload of 50 pages results in a 0% hit rate.

Interestingly, Random fares notably better, not quite approaching optimal, but at least achieving a non-zero hit rate. Turns out that random has some nice properties; one such property is not having weird corner-case behaviors.

## Approximating LRU

Givin that it will be expensive to implement perfect LRU, can we approximate it in some way, and still obtain the desired behavior?

Approximating LRU is more feasible from a computational-overhead standpoint, and indeed it is what many modern systems do. The idea requires some hardware support, in the form of a **use bit** (sometimes called the **reference bit**). There is one use bit per page of the system, and the use bits live in memory somewhere (they could be in the per-process page tables, for example, or just in an array somewhere). Whenever a page is referenced (i.e., read or written), the use bit is set by hardware to 1. The hardware never clears the bit, thought (i.e., sets it to 0); that is the responsibility of the OS.

One such algorithm is the **clock algorithm**. Imagine all the pages of the system arranged in a circular list. A **clock hand** points to some particular page to begin with (it doesn't really matter which). When a replacement must occur, the OS checks if the currently-pointed to page P has a use bit of 1 or 0. If 1, this implies that page P ws recently used and thus is _not_ a good candidate for replacement. Thus, the use bit for P is set to 0 (cleared), and the clock hand is incremented to the next page (P + 1). The algorithm continues until it finds a use bit that is set to 0, implying this page has not been recently used (or, in the worst case, that all pages have been recently used and we have not searched through the entire set of pages, clearing all the bits).

## Considering Dirty Pages

One small modification to the clock algorithm that is commonly made is the additional consideration of whether a page has been modified or not while in memory. The reason for this: if a page has been **modified** and is thus **dirty**, it must be written back to disk to evict it, which is expensive. If it has not been modified (and is thus **clean**), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O. Thus, some VM systems prefer to evict clean pages over dirty pages.

To support this behavior, the hardware should include a **modified bit** (a.k.a. **dirty bit**). This bit is set any time a page is written, and thus can be incorporated into the page-replacement algorithm. The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict first; falling to find those, then for unused pages that are dirty, and so forth.

## Thrashing

What should the OS do when memory is simply oversubscribed, and the memory demands of the set of running processes simply exceeds the available physical memory? In this case, the system will constantly be paging, a condition sometimes referred to as **thrashing**.

Some earlier OS had a fairly sophisticated set of mechanisms to both detect and cope with thrashing when it took place. For example, given a set of processes, a system could decide not to run a subset of processes, with the hope that the reduced set of processes' **working sets** (the pages that they are using actively) fir in memory and thus can make progress. This approach, generally known as **admission control**, states that it is sometimes better to do less work well than to try to do everything at once poorly.

Some current systems take more a draconian approach to memory overload. For example, some versions of Linux run an **out-of-memory killer** when memory is oversubscribed; this daemon chooses a memory-intensive process and kills it, thus reducing memory in a none-too-subtle manner. While successful at reducing memory pressure, this approach can have problems, if, for example, it kills the X server and thus renders any applications requiring the display unusable.
